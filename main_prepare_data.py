"""
File: main_prepare_data.py
M√¥ t·∫£: Script ch√≠nh ch·∫°y B∆∞·ªõc 1 ‚Äì Chu·∫©n b·ªã d·ªØ li·ªáu.
"""
import os
import joblib
import numpy as np
from src.data_loader import download_uci_har_dataset, load_har_data
from src.preprocessing import clean_data, scale_data, segment_data

# ========================== # 0. T·∫°o folder c·∫ßn thi·∫øt # ==========================
os.makedirs("data/raw", exist_ok=True)
os.makedirs("data/processed", exist_ok=True)
os.makedirs("artifacts", exist_ok=True)

# ==========================
# 1. T·∫£i v√† ƒë·ªçc d·ªØ li·ªáu
# ==========================
download_uci_har_dataset()
X_train, y_train, X_test, y_test = load_har_data()

# ==========================
# 2. L√†m s·∫°ch & chu·∫©n h√≥a
# ==========================
X_train = clean_data(X_train)
X_test = clean_data(X_test)

X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)

# L∆∞u scaler ƒë·ªÉ d√πng khi inference
joblib.dump(scaler, "artifacts/scaler.pkl")
print("‚úÖ L∆∞u scaler t·∫°i artifacts/scaler.pkl")

# ==========================
# 3. Ph√¢n ƒëo·∫°n (Windowing)
# ==========================
X_train_w, y_train_w, X_test_w, y_test_w = segment_data(
    X_train_scaled, y_train, X_test_scaled, y_test, window_size=128, overlap=0.5
)

# ==========================
# 4. L∆∞u d·ªØ li·ªáu processed
# ==========================
np.savez_compressed(
    "data/processed/har_data_windows.npz",
    X_train=X_train_w, y_train=y_train_w,
    X_test=X_test_w, y_test=y_test_w
)

print("üéâ Ho√†n t·∫•t b∆∞·ªõc 1 ‚Äì d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u t·∫°i data/processed/har_data_windows.npz")
